# -*- coding: utf-8 -*-
"""Bayesian Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aUs7uqil5B1zeIa5bUWzr9Fp9vnJR86G

**Practical No. 6-A: Implement Bayesian Linear Regression to explore prior and posterior distribution.**

---
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

# Generate synthetic data
np.random.seed(42)
x = np.linspace(-5, 5, 50)
y_true = 2 * x + 1
noise = np.random.normal(0, 2, size=x.shape)
y = y_true + noise
# Define Bayesian Linear Regression functions
def prior_distribution(mean, covariance):
    """Plot the prior distribution."""
    w0, w1 = np.mgrid[-5:5:.1, -5:5:.1]
    pos = np.dstack((w0, w1))
    prior = multivariate_normal(mean, covariance)
    plt.contourf(w0, w1, prior.pdf(pos), cmap="Blues")
    plt.title("Prior Distribution")
    plt.xlabel("w0")
    plt.ylabel("w1")
    plt.colorbar(label="Density")
    plt.show()
# Bayesian update
def posterior_distribution(x, y, prior_mean, prior_cov, likelihood_var):
    """Calculate and plot the posterior distribution."""
    # Design matrix
    X = np.vstack([np.ones_like(x), x]).T

    # Compute posterior covariance and mean
    posterior_cov = np.linalg.inv(
        np.linalg.inv(prior_cov) + (1 / likelihood_var) * X.T @ X
    )
    posterior_mean = posterior_cov @ (
        np.linalg.inv(prior_cov) @ prior_mean + (1 / likelihood_var) * X.T @ y
    )
    w0, w1 = np.mgrid[-5:5:.1, -5:5:.1]
    pos = np.dstack((w0, w1))
    posterior = multivariate_normal(posterior_mean, posterior_cov)

    plt.contourf(w0, w1, posterior.pdf(pos), cmap="Reds")
    plt.title("Posterior Distribution")
    plt.xlabel("w0")
    plt.ylabel("w1")
    plt.colorbar(label="Density")
    plt.show()
    return posterior_mean, posterior_cov

# Prediction using posterior
def predict(x_test, posterior_mean, posterior_cov, likelihood_var):
    """Predict output and plot posterior predictive distribution."""
    X_test = np.vstack([np.ones_like(x_test), x_test]).T
    y_mean = X_test @ posterior_mean
    y_var = np.sum(X_test @ posterior_cov * X_test, axis=1) + likelihood_var
    y_std = np.sqrt(y_var)
    plt.figure(figsize=(8, 6))
    plt.plot(x, y, "o", label="Observed Data")
    plt.plot(x_test, y_mean, label="Predictive Mean", color="red")
    plt.fill_between(
        x_test, y_mean - 2 * y_std, y_mean + 2 * y_std, color="pink", alpha=0.5, label="95% CI"
    )
    plt.legend()
    plt.title("Posterior Predictive Distribution")
    plt.show()
# Define prior parameters
prior_mean = np.array([0, 0])
prior_cov = np.array([[1, 0], [0, 1]])
likelihood_var = 4
# Plot prior\prior_distribution(prior_mean, prior_cov)
# Compute and plot posterior
posterior_mean, posterior_cov = posterior_distribution(
    x, y, prior_mean, prior_cov, likelihood_var
)
# Predict on new data
x_test = np.linspace(-6, 6, 100)
predict(x_test, posterior_mean, posterior_cov, likelihood_var)